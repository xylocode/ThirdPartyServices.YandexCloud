syntax = "proto3";

package yandex.cloud.ai.llm.v1alpha;

import "google/protobuf/wrappers.proto";

option go_package = "github.com/yandex-cloud/go-genproto/yandex/cloud/ai/llm/v1alpha;llm";
option java_package = "yandex.cloud.api.ai.llm.v1alpha";

// Sets the generation options.
message GenerationOptions {
  // Enables streaming of the partially generated text.
  bool partial_results = 1;
  // Affects creativity and randomness of the responses. It is a double number between 0 and infinity. A low temperature causes the responses to be straightforward, a high temperature causes high-level creativity and randomness.
  google.protobuf.DoubleValue temperature = 2;
  // Sets response limit in tokens. The total length of [instruction_text], [request_text], and [max_tokens] should be equal or less than 7400 tokens.
  google.protobuf.Int64Value max_tokens = 3;
}

// Contains the response, its score and length in tokens.
message Alternative {
  // Text of the response.
  string text = 1;
  // Text log likelihood.
  double score = 2;
  // Number of tokens in the response.
  int64 num_tokens = 3;
}

// Contains description of the message for Chat.
message Message {
  // Identifies who sent message. For message from the LLM model, the mandatory value is "assistant".
  string role = 1;
  // Text of the message.
  string text = 2;
}

// Contains description of the LLM token (base coding unit).
message Token {
  // Internal token ID.
  int64 id = 1;
  // Token text representation.
  string text = 2;
  // Type of token (special or not special). Special tokens define the behaviour of the model and are not visible for users.
  bool special = 3;
}
