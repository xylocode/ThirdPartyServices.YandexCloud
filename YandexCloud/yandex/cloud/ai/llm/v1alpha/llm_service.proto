syntax = "proto3";
package yandex.cloud.ai.llm.v1alpha;

import "yandex/cloud/ai/llm/v1alpha/llm.proto";
import "google/api/annotations.proto";
import "yandex/cloud/validation.proto";

option go_package = "github.com/yandex-cloud/go-genproto/yandex/cloud/ai/llm/v1alpha;llm";
option java_package = "yandex.cloud.api.ai.llm.v1alpha";

// Describe your query: set generation options, context in the instruction part and query in the request.
message InstructRequest {

  // Possible value for now: `general`.
  string model = 1 [(length) = "<=50"];

  // Generation options
  GenerationOptions generation_options = 2;

  // Text precondition or context of the request.
  // For example, the instruction is "You are the youngest Nobel laureate", the request text is "Tell us about your daily routine".
  oneof Instruction {

    // Text of the instruction. The total length of [instruction_text], [request_text], and [max_tokens] should be equal or less than 7400 tokens.
    string instruction_text = 3;

    string instruction_uri = 5;
  }

  // Request
  oneof Request {
    // Text of the instruction. The total length of [instruction_text], [request_text], and [max_tokens] should be equal or less than 7400 tokens.
    string request_text = 4;
  }
}

// Contains a set of alternative responses.
message InstructResponse {
  // Generated alternatives
  repeated Alternative alternatives = 1;
  // Number of tokens for instruction text and request text
  int64 num_prompt_tokens = 2;
}

// Describe your chat query: set generation options, context in the instruction part and message history.
message ChatRequest {

  // Possible value for now: `general`.
  string model = 1 [(length) = "<=50"];

  // Generation options
  GenerationOptions generation_options = 2;

  // Text precondition or context of the request. For example, the instruction may be "You are a helpful assistant".
  oneof Instruction {
    // Text of the instruction.
    string instruction_text = 3;
  }

  // Message history.
  repeated Message messages = 4;
}

// Contains a model-generated assistant's response for the chat query.
message ChatResponse {
  // Assistant's message.
  Message message = 1;
  // Total number of tokens for chat request and chat response.
  int64 num_tokens = 2;
}

service TextGenerationService {
  rpc Instruct (InstructRequest) returns (stream InstructResponse) {
    option (google.api.http) = {post: "/llm/v1alpha/instruct" body: "*"};
  }

  rpc Chat (ChatRequest) returns (stream ChatResponse) {
    option (google.api.http) = {post: "/llm/v1alpha/chat" body: "*"};
  }
}

// Tokenization request.
message TokenizeRequest {

  // Possible value for now: `general`.
  string model = 1 [(length) = "<=50"];

  // Text for tokenization request.
  string text = 2;
}

// Tokenization response.
message TokenizeResponse {
  // Sequence of tokens in tokenized text.
  repeated Token tokens = 1;
}

service TokenizerService {
  rpc Tokenize (TokenizeRequest) returns (TokenizeResponse) {
    option (google.api.http) = {post: "/llm/v1alpha/tokenize" body: "*"};
  }
}

message EmbeddingRequest {
  enum EmbeddingType {
    EMBEDDING_TYPE_UNSPECIFIED = 0;
    EMBEDDING_TYPE_QUERY = 1;
    EMBEDDING_TYPE_DOCUMENT = 2;
  }
  EmbeddingType embedding_type = 1;
  string model = 2;
  string text = 3;
}

message EmbeddingResponse {
  repeated double embedding = 1;
  int64 num_tokens = 2;
}

service EmbeddingsService {
  rpc Embedding (EmbeddingRequest) returns (EmbeddingResponse) {
    option (google.api.http) = {post: "/llm/v1alpha/embedding" body: "*"};
  }
}
